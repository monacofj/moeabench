{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "view-in-github",
                "colab_type": "text"
            },
            "source": [
                "<a href=\"https://colab.research.google.com/github/monacofj/moeabench/blob/add-test/misc/benchmark_gallery.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# MoeaBench: Scientific Audit Confrontation\n",
                "This notebook provides a systematic visual audit comparing the **Legacy Heuristic Fronts (v0.6.x)** against the **Analytical Ground Truth (v0.7.5)**.\n",
                "\n",
                "It demonstrates the jump in mathematical precision and identifies the geometric displacement of legacy truths."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Setup Colab: Clone, Enter and Install\n",
                "import os\n",
                "if 'google.colab' in str(get_ipython()):\n",
                "    if not os.path.exists('moeabench'):\n",
                "        !git clone https://github.com/monacofj/moeabench.git\n",
                "    %cd moeabench\n",
                "    !pip install -e .\n",
                "    print(\"Environment ready.\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import os\n",
                "import sys\n",
                "import pandas as pd\n",
                "import numpy as np\n",
                "\n",
                "if os.getcwd() not in sys.path:\n",
                "    sys.path.append(os.getcwd())\n",
                "\n",
                "import MoeaBench as mb\n",
                "from MoeaBench.core import SmartArray"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Visual Auditor Helper\n",
                "Logic to resolve mismatched legacy paths, handle missing analytical sampling (DTLZ8), and plot the confrontation."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def audit_confrontation(mop_name, M=3):\n",
                "    # 1. Setup Problem Instance (Official API)\n",
                "    exp = mb.experiment()\n",
                "    if mop_name.startswith(\"DPF\"):\n",
                "        mop = getattr(mb.mops, mop_name)(M=M, D=M-1)\n",
                "    else:\n",
                "        mop = getattr(mb.mops, mop_name)(M=M)\n",
                "    \n",
                "    # ESSENTIAL: Assign to experiment manager\n",
                "    exp.mop = mop\n",
                "    \n",
                "    # 2. Setup New Truth (v0.7.5)\n",
                "    try:\n",
                "        f_gt_pop = exp.optimal(n_points=1000)\n",
                "        F_gt = f_gt_pop.objs\n",
                "    except (NotImplementedError, AttributeError):\n",
                "        # Fallback for complex cases (DTLZ8)\n",
                "        gt_file = os.path.join(\"tests/ground_truth\", f\"{mop_name}_{M}_optimal.csv\")\n",
                "        if os.path.exists(gt_file):\n",
                "            F_gt = pd.read_csv(gt_file, header=None).values\n",
                "        else:\n",
                "            print(f\"Critical: Analytical truth not available for {mop_name}.\")\n",
                "            return\n",
                "            \n",
                "    gt_smart = SmartArray(F_gt, name=\"v0.7.5 Ground Truth\", \n",
                "                          label=\"Referência Científica (v0.7.5)\")\n",
                "    \n",
                "    # 3. Resolve and Load Legacy (v0.6.x)\n",
                "    prefix = \"lg__\" if \"DTLZ\" in mop_name else \"lg_\"\n",
                "    leg_filename = f\"{prefix}{mop_name}_{M}_opt_front.csv\"\n",
                "    leg_path = os.path.join(\"tests/audit_data\", f\"legacy_{mop_name}\", leg_filename)\n",
                "    \n",
                "    if os.path.exists(leg_path):\n",
                "        F_leg = pd.read_csv(leg_path, header=None).values\n",
                "        leg_smart = SmartArray(F_leg, name=\"v0.6.x Legacy\", \n",
                "                               label=\"Rastro Heurístico (v0.6.x)\")\n",
                "        \n",
                "        # 4. View Confrontation (Passing both SmartArrays to overlay them)\n",
                "        mb.view.topo_shape(gt_smart, leg_smart, \n",
                "                           title=f\"{mop_name} (M={M}): Confronto de Verdades\")\n",
                "    else:\n",
                "        print(f\"Warning: Legacy data not found at {leg_path}.\")\n",
                "        mb.view.topo_shape(gt_smart, title=f\"{mop_name} (M={M}): Ground Truth Only\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. DTLZ Family Audits"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "audit_confrontation(\"DTLZ1\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "audit_confrontation(\"DTLZ2\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "audit_confrontation(\"DTLZ3\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "audit_confrontation(\"DTLZ4\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "audit_confrontation(\"DTLZ5\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "audit_confrontation(\"DTLZ6\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "audit_confrontation(\"DTLZ7\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "audit_confrontation(\"DTLZ8\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "audit_confrontation(\"DTLZ9\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. DPF Family Audits"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "audit_confrontation(\"DPF1\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "audit_confrontation(\"DPF2\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "audit_confrontation(\"DPF3\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "audit_confrontation(\"DPF4\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "audit_confrontation(\"DPF5\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.10.12"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}