{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "view-in-github",
                "colab_type": "text"
            },
            "source": [
                "<a href=\"https://colab.research.google.com/github/monacofj/moeabench/blob/add-test/misc/benchmark_gallery.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# MoeaBench: Scientific Audit Confrontation\n",
                "This notebook provides a systematic visual audit comparing the **Legacy Heuristic Fronts (v0.6.x)** against the **Analytical Ground Truth (v0.7.5)**.\n",
                "\n",
                "It demonstrates the jump in mathematical precision and identifies the geometric displacement of legacy truths."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Setup Colab: Clone, Enter and Install\n",
                "import os\n",
                "if 'google.colab' in str(get_ipython()):\n",
                "    if not os.path.exists('moeabench'):\n",
                "        !git clone https://github.com/monacofj/moeabench.git\n",
                "    %cd moeabench\n",
                "    !pip install -e .\n",
                "    print(\"Environment ready.\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import os\n",
                "import sys\n",
                "import pandas as pd\n",
                "import numpy as np\n",
                "\n",
                "if os.getcwd() not in sys.path:\n",
                "    sys.path.append(os.getcwd())\n",
                "\n",
                "import MoeaBench as mb\n",
                "from MoeaBench.core import SmartArray"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Visual Auditor Helper\n",
                "Logic to resolve mismatched legacy paths, handle missing analytical sampling (DTLZ8), and plot the confrontation."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def audit_confrontation(mop_name, M=3):\n",
                "    # 1. Setup Problem Instance\n",
                "    exp = mb.experiment()\n",
                "    if mop_name.startswith(\"DPF\"):\n",
                "        exp.mop = getattr(mb.mops, mop_name)(M=M, D=M-1)\n",
                "    else:\n",
                "        exp.mop = getattr(mb.mops, mop_name)(M=M)\n",
                "    \n",
                "    # 2. Setup New Truth (v0.7.5)\n",
                "    # Try analytical generation first; fallback to CSV for complex cases (DTLZ8)\n",
                "    try:\n",
                "        f_gt_pop = exp.optimal(n_points=500)\n",
                "        F_gt = f_gt_pop.objs\n",
                "    except (NotImplementedError, AttributeError):\n",
                "        # Load from frozen Ground Truth directory\n",
                "        gt_file = os.path.join(\"tests/ground_truth\", f\"{mop_name}_{M}_optimal.csv\")\n",
                "        if os.path.exists(gt_file):\n",
                "            F_gt = pd.read_csv(gt_file, header=None).values\n",
                "        else:\n",
                "            print(f\"Critical: Analytical truth not available for {mop_name}.\")\n",
                "            return\n",
                "            \n",
                "    gt_smart = SmartArray(F_gt, name=\"v0.7.5 Ground Truth\", \n",
                "                          label=\"Referência Científica\")\n",
                "    \n",
                "    # 3. Resolve and Load Legacy (v0.6.x)\n",
                "    prefix = \"lg__\" if \"DTLZ\" in mop_name else \"lg_\"\n",
                "    leg_filename = f\"{prefix}{mop_name}_{M}_opt_front.csv\"\n",
                "    leg_path = os.path.join(\"tests/audit_data\", f\"legacy_{mop_name}\", leg_filename)\n",
                "    \n",
                "    if os.path.exists(leg_path):\n",
                "        F_leg = pd.read_csv(leg_path, header=None).values\n",
                "        leg_smart = SmartArray(F_leg, name=\"v0.6.x Legacy\", \n",
                "                               label=\"Heurístico (Rastro)\")\n",
                "        \n",
                "        # 4. View Confrontation\n",
                "        mb.view.topo_shape(gt_smart, leg_smart, \n",
                "                           title=f\"{mop_name} (M={M}): Confronto de Verdades\")\n",
                "    else:\n",
                "        print(f\"Warning: Legacy data for {mop_name} not found. Showing only GT.\")\n",
                "        mb.view.topo_shape(gt_smart, title=f\"{mop_name} (M={M}): Ground Truth\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. DTLZ Audit Loop"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "for i in range(1, 10):\n",
                "    print(f\"Rendering {f'DTLZ{i}'}...\")\n",
                "    audit_confrontation(f\"DTLZ{i}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. DPF Audit Loop"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "for i in range(1, 6):\n",
                "    print(f\"Rendering {f'DPF{i}'}...\")\n",
                "    audit_confrontation(f\"DPF{i}\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.10.12"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}