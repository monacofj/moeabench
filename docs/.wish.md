<!--
SPDX-FileCopyrightText: 2025 Monaco F. J. <monaco@usp.br>
SPDX-FileCopyrightText: 2025 Silva F. F. <fernandoferreira.silva42@usp.br>

SPDX-License-Identifier: GPL-3.0-or-later
-->

# Future Directions: Enhancing MoeaBench's Statistical Narrative

To enrich the scientific narrative of MoeaBench and elevate the technical discourse the framework facilitates, we envision several statistical additions. These enhancements focus on multi-algorithm comparison, the nuances of stochastic robustness, and the underlying geometry of the search process. Aligned with our **Technical Storytelling** philosophy, these features aim to transform raw data into a compelling scientific journey.

## 1. Multi-Algorithm Analysis: The "Arena" Narrative

While MoeaBench currently excels at pairwise comparisons through tools like Mann-Whitney and the KS-test, comprehensive benchmarking often demands a broader perspective. We need to determine if, out of a group of multiple algorithms, any real difference exists or if the results are merely artifacts of stochastic noise.

To address this, we propose integrating the **Friedman Test** paired with **Nemenyi Post-hoc** analysis. The Friedman test provides a global view of the group's performance variance. If global significance is detected, the post-hoc analysis identifies the specific pairs of algorithms that truly diverge. This would be visually anchored by **Critical Difference (CD) Diagrams**, providing the definitive "battle map" for any research paper.

## 2. Performance Profiles: The "Efficiency" Narrative

A persistent challenge in optimization is dealing with algorithms that show high average performance but suffer from poor robustness in outlier cases. A simple mean or median often masks these critical failure modes.

We believe that **Dolan-Moré Performance Profiles** are the solution. By plotting the cumulative probability that a solver's performance remains within a factor of the best-observed result, we can craft far more nuanced conclusions. We could state, for instance, that while one algorithm has an 80% chance of being the fastest, another might be the more reliable choice for guaranteeing at least 90% of the maximum Hypervolume in every single run.

## 3. Stability Dynamics: The "Reliability" Narrative

MoeaBench's commitment to documenting the "scientific journey" implies that we should not only report final results but also the evolution of reliability.

We plan to introduce **Confidence Bands** within the `timeplot` convention. By surrounding the median Hypervolume with an **Interquartile Range (IQR)** cloud, we can visually pinpoint the exact transition from exploratory search to stable convergence. Furthermore, metrics like **Success Rate** and **Time-to-Target (TTT)** will allow us to frame performance not just as a matter of "quality," but as one of "cost-effectiveness"—measuring the generation-count "investment" required to reach a specific performance threshold.

## 4. Geometry of Diversity: The "Architecture" Narrative

The existing `strata` analysis provides excellent insight into selection pressure, but the internal "geology" of solution distribution remains a frontier to explore.

We suggest implementing **Hypervolume Contribution (HVC)** to identify the "anchor solutions" that are essential to the front's quality versus those that are redundant. Additionally, **Schott's Spacing (S)** will serve as a vital diagnostic tool for measuring the uniformity of the distribution, helping researchers identify if an algorithm is suffering from "premature clustering" or "diversity loss" in specific regions of the objective space.

## 5. Attainment Divergence: The "Stochastic Geology" Narrative

Expanding the logic found in our `attainment.py` module, we see an opportunity to quantify algorithmic risk through **Attainment Spread**. 

By calculating the distance (using Earth Mover's Distance or Euclidean metrics) between the 5% attainment surface—representing the rare "best case"—and the 95% surface—representing the "common worst case"—we can offer a formal measure of predictability. A smaller spread indicates an algorithm with a high Degree of Certainty, a critical factor for researchers deploying these tools in high-stakes environments.
